#!/usr/bin/env python3
from __future__ import annotations

import argparse
import json
import math
import os
import subprocess
import sys
from pathlib import Path
from typing import Iterable, List, Tuple, Dict, Any


def ensure_pymupdf() -> None:
    try:
        import fitz  # noqa: F401
    except Exception:
        print("[pdf-to-pages] Installing PyMuPDF...", file=sys.stderr)
        subprocess.check_call([sys.executable, "-m", "pip", "install", "--quiet", "PyMuPDF>=1.23.7"])  # type: ignore


def parse_page_range(spec: str | None, page_count: int) -> List[int]:
    if not spec:
        return list(range(1, page_count + 1))
    pages: set[int] = set()
    for part in spec.split(","):
        part = part.strip()
        if "-" in part:
            a, b = part.split("-", 1)
            start = int(a)
            end = int(b)
            for p in range(start, end + 1):
                if 1 <= p <= page_count:
                    pages.add(p)
        else:
            p = int(part)
            if 1 <= p <= page_count:
                pages.add(p)
    return sorted(pages)


def _env_float(key: str, default: float) -> float:
    try:
        return float(os.environ.get(key, str(default)))
    except Exception:
        return default


def _env_int(key: str, default: int) -> int:
    try:
        return int(os.environ.get(key, str(default)))
    except Exception:
        return default


def _median(vals: List[float]) -> float:
    if not vals:
        return 0.0
    s = sorted(vals)
    n = len(s)
    mid = n // 2
    if n % 2 == 1:
        return float(s[mid])
    return float((s[mid - 1] + s[mid]) / 2.0)


def _is_bullet(text: str) -> bool:
    import re

    return bool(re.match(r"^\s*(?:[\*\-\u2022\u25E6]|\d+[\.)])\s+", text))


def _merge_lines_to_paragraphs(lines: List[Dict[str, Any]], cfg: Dict[str, float]) -> List[Dict[str, Any]]:
    if not lines:
        return []
    # Sort lines by y then x
    lines = sorted(lines, key=lambda ln: (round(ln["bbox"][1], 2), round(ln["bbox"][0], 2)))
    heights = [ln["bbox"][3] - ln["bbox"][1] for ln in lines]
    sizes = [ln.get("size", 0.0) for ln in lines]
    median_h = _median(heights) or 12.0
    median_size = _median(sizes) or 12.0

    paras: List[Dict[str, Any]] = []
    cur: Dict[str, Any] | None = None

    def start_para(ln: Dict[str, Any], btype: str = "paragraph") -> Dict[str, Any]:
        return {
            "bbox": list(ln["bbox"]),
            "content": ln["text"].strip(),
            "block_type": btype,
            "confidence": None,
            "_last_left": ln["bbox"][0],
            "_last_bottom": ln["bbox"][3],
        }

    for ln in lines:
        left, top, right, bottom = ln["bbox"]
        gap = top - (cur["_last_bottom"] if cur else top)
        left_diff = abs((cur["_last_left"] if cur else left) - left)
        is_heading = ln.get("size", 0.0) >= cfg["HEAD_FACTOR"] * median_size and not _is_bullet(ln["text"])  # big text
        is_bullet = _is_bullet(ln["text"])  # list marker

        new_para = False
        para_type = "paragraph"
        if cur is None:
            new_para = True
        else:
            # Large vertical gap indicates paragraph break
            if gap > cfg["LINE_GAP_FACTOR"] * median_h:
                new_para = True
            # Significant left indent/outdent change suggests new paragraph
            elif left_diff > cfg["LEFT_EPS_PX"] and left - cur["_last_left"] > cfg["INDENT_PX"]:
                new_para = True
            # Bullet or heading should start new block
            elif is_bullet or is_heading:
                new_para = True

        if new_para:
            if is_heading:
                para_type = "heading"
            elif is_bullet:
                para_type = "list-item"
            cur = start_para(ln, para_type)
            paras.append(cur)
        else:
            # Merge into current paragraph; handle hyphenation
            prev_text = cur["content"].rstrip()
            nxt = ln["text"].lstrip()
            if prev_text.endswith("-") and nxt and nxt[:1].islower():
                cur["content"] = prev_text[:-1] + nxt
            else:
                # Add space unless previous ends with hyphen or hard break
                sep = " " if prev_text and not prev_text.endswith("-") else ""
                cur["content"] = (prev_text + sep + nxt).strip()
            # Expand bbox
            cur["bbox"][0] = min(cur["bbox"][0], left)
            cur["bbox"][1] = min(cur["bbox"][1], top)
            cur["bbox"][2] = max(cur["bbox"][2], right)
            cur["bbox"][3] = max(cur["bbox"][3], bottom)
            cur["_last_left"] = left
            cur["_last_bottom"] = bottom

    # Strip temp keys
    for p in paras:
        p.pop("_last_left", None)
        p.pop("_last_bottom", None)
    return paras


def _extract_blocks_heuristic(page) -> List[Dict[str, Any]]:  # type: ignore
    raw = page.get_text("rawdict")
    blocks: List[Dict[str, Any]] = []
    # Config thresholds
    cfg = {
        "LINE_GAP_FACTOR": _env_float("PARA_LINE_GAP", 1.3),
        "INDENT_PX": _env_float("PARA_INDENT_PX", 18.0),
        "LEFT_EPS_PX": _env_float("LEFT_EPSILON_PX", 8.0),
        "HEAD_FACTOR": _env_float("HEAD_SIZE_FACTOR", 1.3),
    }

    for blk in raw.get("blocks", []):
        if blk.get("type") != 0:
            continue
        # Build lines with bbox, text, and avg size
        lines = []
        for ln in blk.get("lines", []):
            bbox = ln.get("bbox", [0, 0, 0, 0])
            spans = ln.get("spans", [])
            text = "".join(s.get("text", "") for s in spans).strip()
            if not text:
                continue
            # avg span size
            sizes = [float(s.get("size", 0.0)) for s in spans if s.get("size")]
            avg_size = sum(sizes) / len(sizes) if sizes else 0.0
            lines.append({"bbox": bbox, "text": text, "size": avg_size})
        merged = _merge_lines_to_paragraphs(lines, cfg)
        blocks.extend(merged)
    return blocks


def _fallback_blocks_simple(page) -> List[Dict[str, Any]]:  # type: ignore
    # Simple fallback: use PyMuPDF 'blocks' and treat each as a paragraph
    out: List[Dict[str, Any]] = []
    for blk in page.get_text("blocks") or []:
        if len(blk) < 5:
            continue
        x0, y0, x1, y1, txt = blk[:5]
        text = (txt or "").strip()
        if not text:
            continue
        out.append({
            "bbox": [x0, y0, x1, y1],
            "content": text,
            "block_type": "paragraph",
            "confidence": None,
        })
    # Sort by reading order
    out.sort(key=lambda b: (round(b["bbox"][1], 2), round(b["bbox"][0], 2)))
    return out


def render_and_extract(pdf_path: Path, out_root: Path, dpi: int, pages: Iterable[int], mode: str) -> dict:
    import fitz  # type: ignore

    out_pages = out_root / "pages"
    out_pages.mkdir(parents=True, exist_ok=True)
    doc = fitz.open(pdf_path)
    total_blocks = 0
    meta = {
        "source": str(pdf_path),
        "page_count": doc.page_count,
        "dpi": dpi,
        "mode": mode,
        "pages": [],
        "total_blocks": 0,
    }
    for pno in pages:
        page = doc.load_page(pno - 1)
        # Render to PNG
        zoom = dpi / 72.0
        mat = fitz.Matrix(zoom, zoom)
        pix = page.get_pixmap(matrix=mat, alpha=False)
        png_path = out_pages / f"page-{pno:04d}.png"
        pix.save(png_path)

        # Extract blocks (paragraph-like) via heuristics; fallback to simple blocks if empty
        norm_blocks = _extract_blocks_heuristic(page)
        if not norm_blocks:
            norm_blocks = _fallback_blocks_simple(page)
        json_path = out_pages / f"page-{pno:04d}.blocks.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump({"page": pno, "blocks": norm_blocks}, f, ensure_ascii=False, indent=2)
        total_blocks += len(norm_blocks)
        meta["pages"].append({"page": pno, "png": str(png_path), "blocks_json": str(json_path), "blocks_count": len(norm_blocks)})
    # Write summary
    meta["total_blocks"] = total_blocks
    with open(out_root / "meta.json", "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    return meta


def main() -> int:
    ap = argparse.ArgumentParser(description="PDF â†’ per-page PNG + block JSON via PyMuPDF")
    ap.add_argument("input", help="PDF file path")
    ap.add_argument("--outdir", default=os.environ.get("OCR_DIR", "/data/ocr"), help="Output root (will create a subdir)")
    ap.add_argument("--subdir", help="Optional subdir name under outdir (e.g., a hash)")
    ap.add_argument("--dpi", type=int, default=300)
    ap.add_argument("--mode", choices=["auto", "rasterize", "extract-text"], default="auto")
    ap.add_argument("--page-range", help="e.g. 1-3,5,7")
    ap.add_argument("--dry-run", action="store_true")
    args = ap.parse_args()

    pdf = Path(args.input)
    out_root = Path(args.outdir)
    if args.subdir:
        out_root = out_root / args.subdir
    out_root.mkdir(parents=True, exist_ok=True)

    print(f"[pdf-to-pages] input={pdf} out={out_root} dpi={args.dpi} mode={args.mode} range={args.page_range}")
    if args.dry_run:
        print("[pdf-to-pages] Dry run; not producing outputs.")
        return 0

    ensure_pymupdf()
    import fitz  # noqa: F401

    # Determine pages
    doc = fitz.open(pdf)
    pages = parse_page_range(args.page_range, doc.page_count)
    doc.close()

    meta = render_and_extract(pdf, out_root, args.dpi, pages, args.mode)
    print(f"[pdf-to-pages] Wrote {len(meta['pages'])} pages under {out_root}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
